{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c61f54",
   "metadata": {},
   "source": [
    "# Introduction to JumpStart - Depth guided image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc23bae",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to Amazon [SageMaker JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html)! You can use Sagemaker JumpStart to solve many Machine Learning tasks through one-click in SageMaker Studio, or through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html#use-prebuilt-models-with-sagemaker-jumpstart).\n",
    "\n",
    "In this demo notebook, we introduce a new feature that enables users to generate depth aware images with Stable Diffusion models. You can generate images radically different from an existing image while preserving structural coherence and depth. This can be useful in a variety of applications including product design, digital advertisements, interior design, landscape design, style transfer from one image to another, generating image in a specific pose.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db28351",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Run inference on the pre-trained model](#2.-run-inference-on-the-pre-trained-model)\n",
    "3. [Query endpoint and parse response](#3.-Query-endpoint-and-parse-response)\n",
    "4. [Use Cases](#4.-Use-Cases)\n",
    "5. [Impact of parameters on performance](#5.-Impact-of-parameters-on-performance)\n",
    "6. [Clean up the endpoint](#6.-Clean-up-the-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce462973",
   "metadata": {},
   "source": [
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "Note: This notebook requires an accelerated computing instance to deploy the model. Please make sure you have sufficient quota to execute the notebook.\n",
    "\n",
    "Note: After you’re done running the notebook, make sure to delete all resources so that all the resources that you created in the process are deleted and your billing is stopped. Code in [Clean up the endpoint](#6.-Clean-up-the-endpoint) deletes model and endpoints that are created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea47727",
   "metadata": {},
   "source": [
    "### 1. Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b91e81",
   "metadata": {},
   "source": [
    "---\n",
    "Before executing the notebook, there are some initial steps required for set up. This notebook requires ipywidgets and latest version of sagemaker.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25293522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48370155",
   "metadata": {},
   "source": [
    "#### Permissions and environment variables\n",
    "\n",
    "---\n",
    "To host on Amazon SageMaker, we need to set up and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook as the AWS account role with SageMaker access. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90518e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "aws_role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe28505-a446-4a39-bd0d-7750d0099d5a",
   "metadata": {},
   "source": [
    "### 2. Run inference on the pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca670532-d3a0-4641-9ea3-aea538a16d47",
   "metadata": {},
   "source": [
    "#### 2.1. Select a Model\n",
    "\n",
    "***\n",
    "You can continue with the default model, or can choose a different model from the dropdown generated upon running the next cell. A complete list of SageMaker pre-trained models can also be accessed at Sagemaker [pre-trained Models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html#). For this lab, we recommend using the default model_id.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc2e89d-4ed8-4380-a17c-876474e75817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Retrieves all Text-to-Image generation models.\n",
    "filter_value = \"task == depth2img\"\n",
    "depth2img_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=depth2img_models,\n",
    "    value=\"model-depth2img-stable-diffusion-2-depth-fp16\",\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "display(model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a90c7d-d5cd-41c7-8e41-e1b96f841cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = model_dropdown.value, \"*\"\n",
    "\n",
    "is_controlnet_model= (model_id != \"model-depth2img-stable-diffusion-2-depth-fp16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e057eb3-ead2-4919-9c96-545073329b8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2. Retrieve JumpStart Artifacts & Deploy an Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc6436-d22a-4aed-b857-6a004d58f710",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Using SageMaker, we can perform inference on the pre-trained model, even without fine-tuning it first on a new dataset. We start by retrieving the `deploy_image_uri`, `deploy_source_uri`, and `model_uri` for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it. This may take a few minutes.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a79ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters, instance_types\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "# Instances with more GPU memory supports generation of larger images.\n",
    "inference_instance_type = instance_types.retrieve_default(\n",
    "    region=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri. This includes the pre-trained model and parameters as well as the inference scripts.\n",
    "# This includes all dependencies and scripts for model loading, inference handling etc..\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.4xlarge\",\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665dfbe4-2857-484e-8179-3ad11307822c",
   "metadata": {},
   "source": [
    "### 3. Query endpoint and parse response\n",
    "\n",
    "---\n",
    "Input to the endpoint is a prompt, an  image and image generation parameters in json format and encoded in `utf-8` format. Output of the endpoint is a `json` with generated images and the input prompt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5f775-c73c-4a19-a581-9f9db30238d6",
   "metadata": {},
   "source": [
    "We start by writing some helper function for querying the endpoint, parsing the response and display generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d831a-da65-4092-bb9f-eeffb1d3109d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import List, Union\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def query(model_predictor, payload):\n",
    "    \"\"\"Query the model predictor.\"\"\"\n",
    "    query_response = model_predictor.predict(\n",
    "        payload,\n",
    "        {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    \"\"\"Parse response and return the generated images and prompt.\"\"\"\n",
    "\n",
    "    response_dict = json.loads(query_response)\n",
    "    return response_dict[\"generated_images\"]\n",
    "\n",
    "\n",
    "def display_img_and_titles(img_list: List[Union[str,Image.Image]], titles: List[str], num_images_per_row:int = 1):\n",
    "    \"\"\"Display images.\n",
    "    \n",
    "    img: can be a list of image names or an image\n",
    "    titles: list of strings.\n",
    "    \"\"\"\n",
    "    f= plt.figure(figsize=(30,30))\n",
    "\n",
    "    for i in range(len(img_list)):\n",
    "        img, title = img_list[i], titles[i]\n",
    "        if isinstance(img, str):\n",
    "            img = Image.open(img).convert(\"RGB\")\n",
    "\n",
    "        if i%num_images_per_row == 0:\n",
    "            if i>0:\n",
    "                plt.show(block=True)\n",
    "                f = plt.figure(figsize=(30,30))\n",
    "        \n",
    "        ax = f.add_subplot(1, num_images_per_row, i%num_images_per_row+1)\n",
    "        plt.imshow(img)\n",
    "        ax.title.set_text(title)\n",
    "        ax.axis(\"off\")\n",
    "        i +=1\n",
    "    \n",
    "    plt.show(block=True)\n",
    "\n",
    "def download_image_from_jumpstart_bucket(input_img_file_name):\n",
    "    region = boto3.Session().region_name\n",
    "    s3_bucket = f\"jumpstart-cache-prod-{region}\"\n",
    "    key_prefix = \"model-metadata/assets\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.download_file(s3_bucket, f\"{key_prefix}/{input_img_file_name}\", input_img_file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea0434b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "Below, we put in the example input image and a prompt. You can put in any text and any image and the model generates the corresponding image with similar spatial features. \n",
    "\n",
    "You may also test it with your own images! Simply put the image into the folder _images/input/_. Your output image will be saved in _images/output_, download it to your local machine to save it\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395840cf-1f60-4914-8e09-8c114127febf",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"images/output\").mkdir(exist_ok=True)\n",
    "\n",
    "def download_query_parse_response_and_display(input_img_file_name, parameters, num_images_per_row=2, original_image_display_title = \"original\", generated_image_display_title: Union[str, List[str]] = \"generated\", override_parameter_choices = {}, skip_display=False):\n",
    "    \n",
    "    # endpoint expects payload to be a json with the low resolution jpeg image as bytes encoded with base64.b64 encoding.\n",
    "    with open(input_img_file_name, \"rb\") as f:\n",
    "        input_image_bytes = f.read()\n",
    "    encoded_image = base64.b64encode(bytearray(input_image_bytes)).decode()\n",
    "    payload = parameters.copy()\n",
    "    payload[\"image\"] = encoded_image\n",
    "    generated_images = []\n",
    "    if override_parameter_choices:\n",
    "        for parameter, parameter_choices in override_parameter_choices.items():\n",
    "            for parameter_choice in parameter_choices:\n",
    "                payload[parameter] = parameter_choice\n",
    "                query_response = query(model_predictor, json.dumps(payload).encode(\"utf-8\"))\n",
    "                generated_images += parse_response(query_response)\n",
    "    else:\n",
    "        query_response = query(model_predictor, json.dumps(payload).encode(\"utf-8\"))\n",
    "        # endpoint returns the jpeg image as bytes encoded with base64.b64 encoding.\n",
    "        generated_images= parse_response(query_response)\n",
    "\n",
    "    generated_images_rgb = []\n",
    "    count = 1\n",
    "    for generated_image in generated_images:\n",
    "        generated_image_decoded = BytesIO(base64.b64decode(generated_image.encode()))\n",
    "        generated_images_rgb.append(Image.open(generated_image_decoded).convert(\"RGB\"))\n",
    "        \n",
    "        \n",
    "        filename = input_img_file_name.split(\".\")[0].split(\"/\")[-1] + str(count) + \".jpg\"\n",
    "        count+=1\n",
    "        \n",
    "        image_path = f'images/output/{filename}' \n",
    "        decoded_image_data = base64.b64decode(generated_image)\n",
    "        with open(image_path, 'wb+') as file:\n",
    "            file.write(decoded_image_data)\n",
    "\n",
    "        \n",
    "    if isinstance(generated_image_display_title, str):\n",
    "        generated_image_display_title = [generated_image_display_title]*len(generated_images_rgb)\n",
    "    if not skip_display:\n",
    "        display_img_and_titles([input_img_file_name] + generated_images_rgb, [original_image_display_title]+generated_image_display_title, num_images_per_row=num_images_per_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd61736-30de-4bde-85ee-c1650c312e57",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = 'room.jpg'\n",
    "input_img_file_name = f'images/input/{filename}'\n",
    "\n",
    "parameters = {\n",
    "    \"prompt\": \"contemporary style,  marble floor\",\n",
    "    \"num_inference_steps\": 50,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"num_images_per_prompt\":2,\n",
    "}\n",
    "\n",
    "download_query_parse_response_and_display(input_img_file_name, parameters,num_images_per_row=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206347c3-62cb-4bda-b6a7-7541b4ec5e76",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "### Supported parameters\n",
    "\n",
    "***\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **prompt**: prompt to guide the image generation. Must be specified and can be a string or a list of strings.\n",
    "* **image**: The original image.\n",
    "* **num_inference_steps**  (optional): number of denoising steps during image generation. More steps lead to higher quality image. If specified, it must a positive integer.\n",
    "* **guidance_scale**  (optional): higher guidance scale results in image closely related to the prompt, at the expense of image quality. If specified, it must be a float. guidance_scale<=1 is ignored.\n",
    "* **negative_prompt** (optional): guide image generation against this prompt. If specified, it must be a string or a list of strings and used with guidance_scale. If guidance_scale is disabled, this is also disabled. Moreover, if prompt is a list of strings then negative_prompt must also be a list of strings.\n",
    "* **num_images_per_prompt**  (optional): number of images returned per prompt. If specified it must be a positive integer.\n",
    "* **seed**: fix the randomized state for reproducibility. If specified, it must be an integer.\n",
    "* **batch_size** (optional): Number of images to generate in a single forward pass. If using a smaller instance or generating many images, please reduce batch_size to be a small number (1-2). Number of images = number of prompts*num_images_per_prompt.\n",
    "* **strength** (optional, only for sd-depth, not applicable to controlnet depth): Amount of noise to add the original image initially. If specified, it must be between 0 and 1. If strength is 1, maximum noise will be added to the input image before denoising process starts and it effectively ignores the input image except the depth map. If strength is 0, no noise is added to the input image before the denoising process starts.\n",
    "* **scheduler** (optional): Scheduler (also known as sampler) to use during the de-noising process. It controls the tradeoff between de-noising speed and de-noising quality. You are encouraged to try different schedulers to figure out which works best for your purpose. If specified, it must be from the following list [`PNDMScheduler`, `EulerAncestralDiscreteScheduler`, `KDPM2AncestralDiscreteScheduler`, `UniPCMultistepScheduler`, `DEISMultistepScheduler`, `DDIMScheduler`, `KDPM2DiscreteScheduler`, `EulerDiscreteScheduler`, `HeunDiscreteScheduler`, `DDPMScheduler`]. Note that once you change the scheduler, all subsequent inference calls will use that scheduler. You can change the scheduler again by setting different value for the scheduler. To learn more, please see [this documentation](https://huggingface.co/docs/diffusers/using-diffusers/schedulers) and the [blog post](https://stable-diffusion-art.com/samplers/).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0284aa17-2144-4436-a4eb-a603d5ec35a1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = { \n",
    "    \"prompt\":\"European style, marble floor, minimalist lifestyle, nature and wood, magical house\",\n",
    "    \"num_inference_steps\":30,\n",
    "    \"guidance_scale\":7.5,\n",
    "    \"negative_prompt\":\"poor quality\",\n",
    "    \"num_images_per_prompt\":2,\n",
    "    \"seed\": 1,\n",
    "    \"batch_size\":2,\n",
    "    \"strength\":0.5,\n",
    "    \"scheduler\": \"DDIMScheduler\"\n",
    "}\n",
    "download_query_parse_response_and_display(input_img_file_name, parameters, num_images_per_row=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ae1b3-dcef-4386-8e82-e8def7ee1cef",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "### 4. Use Cases\n",
    "***\n",
    "\n",
    "Stable Diffusion Depth-to-Image can be useful in a variety of creative applications and generate images radically different from the original while preserving the coherence and depth. It takes away the hassle of having to do extensive Photoshop for idea exploration. Simply use a short description to guide the image generation and a new novel image is presented to you within seconds. Here are some possible use-cases.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab9ad9-3015-412d-a929-11526e13df01",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "#### 4.1. Marketing and Branding\n",
    "***\n",
    "You are taking photos of your product to be placed in digital advertisements or brochures and is tasked to come up with a photo that brings a unique ‘feel and message’ to the audience. The original photos are good but lacks creativity. Pass your photo to Depth-to-Image with interesting prompts and see how it can generate intriguing ideas for you. Here is an example of how a photo of a simple beverage can be elevated into a stunning photo.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5939cb-5dc9-4d51-a2e2-9a9c4a7336e1",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"beverage.jpg\"\n",
    "input_img_file_name = f'images/input/{filename}'\n",
    "\n",
    "parameters = { \n",
    "    \"prompt\":\"a glass of cocktail, intimate and romantic ambience\",\n",
    "    \"seed\": 2,\n",
    "    \"strength\":0.7,\n",
    "    \"num_inference_steps\": 100,\n",
    "    \"num_images_per_prompt\": 5,\n",
    "    \"batch_size\":2\n",
    "}\n",
    "\n",
    "download_query_parse_response_and_display(input_img_file_name, parameters, num_images_per_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edaea5-d6f2-4107-91cf-fcf2b332e559",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "source": [
    "A slightly lower than 1 denoising strength help to retain similarity of the original photo, while giving Depth-to-Image sufficient room to generate novel ideas. By retaining the characteristics of the original beverage, it make these ideas feasible to be executed on your product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ba9e1-70eb-4cca-b698-c361c25768fa",
   "metadata": {},
   "source": [
    "#### 4.2. Interior Designs\n",
    "***\n",
    "Depth-to-Image works really well on exploring different interior design styles while keeping the interior space and boundaries coherent with your input image. This enables you to quickly do a mock-up of how the space will look in many different styles. You are also able to specify specific features you wish to see in your space.  Here are some examples we have generated. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080ac47-6588-4df4-bc10-0756222398e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"room.jpg\"\n",
    "input_img_file_name = f'images/input/{filename}'\n",
    "\n",
    "parameters = { \n",
    "    \"prompt\":\"Scandinavian style, majestic and luxurious, chandelier lights, warm lighting\",\n",
    "    \"seed\": 1,\n",
    "    \"strength\":1,\n",
    "    \"num_images_per_prompt\":5,\n",
    "    \"batch_size\":2\n",
    "}\n",
    "download_query_parse_response_and_display(input_img_file_name, parameters, num_images_per_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ac04b-692a-4300-94ff-1f14dd9198e5",
   "metadata": {},
   "source": [
    "Here are some prompts for you to try out: “European style, marble floor, minimalist lifestyle, nature and wood, magical house”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17287891-b61b-4331-887f-52046a5d36f9",
   "metadata": {},
   "source": [
    "### 4.3. Game Development\n",
    "\n",
    "***\n",
    "Graphics and themes in a game can have a huge impact on players’ experience. To make a game more captivating, companies strive to create the most appealing in-game landscapes. Using Depth-to-Image, you can provide a base image that contains some elements that you want to include, and generate an image that have entirely different style. Here are some of the examples:\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b598ea8-3cdf-4a11-b9d3-9565f01deefa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = \"mountain.jpg\"\n",
    "input_img_file_name = f'images/input/{filename}'\n",
    "\n",
    "parameters = { \n",
    "    \"prompt\": \"a dragon mountain range and river, magical hut, dark and stormy\",\n",
    "    \"seed\": 1,\n",
    "    \"strength\":0.75,\n",
    "    \"num_images_per_prompt\":5,\n",
    "    \"batch_size\":2\n",
    "}\n",
    "download_query_parse_response_and_display(input_img_file_name, parameters, num_images_per_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d1ced-7111-40cc-b524-4e931fb63dc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 5. Impact of parameters on performance\n",
    "In this section, we will explore what are the impact the parameters will have on the generated images. Understanding the impacts can help us in guiding the generation to our desired goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90704911-74dd-4a89-a817-3e0213398644",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example images on which to do evaluation\n",
    "\n",
    "example_images_and_prompts = [\n",
    "    [\"images/input/mountain.jpg\", \"god of thunder, mysterious cottage, snowy mountains with ice golems, ultra realistic, sci-fi movie\"],\n",
    "    [\"images/input/room.jpg\", \"European style, marble floor, minimalist lifestyle, nature and wood, magical house\" ],\n",
    "    [\"images/input/beverage.jpg\", \"a glass of cocktail with intimate and romantic ambience\"],\n",
    "    [\"images/input/bottle.jpg\", \"modern fragrance scent bottle, flower petals and baby breath, ultra realistic, 8k, scandinavian style, warm wood background\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ce84f-6d1b-439b-b615-fd0d95290883",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.1. Strength\n",
    "\n",
    "***\n",
    "_Note that strength is not applicable to Controlnet based depth-2-image models_.\n",
    "\n",
    "Strength determine the amount of noise, controlled by denoising strength, added to the image based on a seed. A value of 0 will add no noise to the orginal image, while a value of 1 will completely replaced the original image with noise.\n",
    "\n",
    "The strength parameter can be used to control how much the output image resembles the original image. Using depth-to-image will help ensure that the objects in the original image will retain its shape and size even with a strength of 1. To completely change the style of an object, set the strength to 1. If you wish to retain some characteristics of the original object, a strength of 0.3 to 0.5 is recommended.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98d6d30-7d73-4c9e-975c-7a9e80e2e3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for controlnet models and skip this section\n",
    "is_controlnet_model = False\n",
    "\n",
    "if not is_controlnet_model:\n",
    "    for example_img, prompt in example_images_and_prompts:\n",
    "        parameters = {\"prompt\": prompt, \"seed\":1}\n",
    "        strength_choices = [0.1,0.3,0.5,0.7,1]\n",
    "        download_query_parse_response_and_display(example_img, parameters, num_images_per_row=3, \n",
    "                                                  override_parameter_choices = {\"strength\":strength_choices}, \n",
    "                                                 generated_image_display_title = [f\"strength:{strength}\" for strength in strength_choices]\n",
    "                                                 )\n",
    "else:\n",
    "    print(\"Strength is not an applicable parameter for controlnet models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766bd297-92d4-4326-b01b-d91b55d8d1e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.2. Guidance Scale/CFG scale\n",
    "\n",
    "***\n",
    "Guidance Scale controls how much influence the prompt will have on the image generation process. This parameter can range from -999 to 999, with higher values giving the prompt more influence. A negative value will simply make the prompt work as a \"negative prompt\" instead. The common practice for this parameter value will be between 1 to 30. For any values below 0, negative prompt should be used instead, while any values above 30 will likely result in an over-contrasted image.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83fac4-39d4-429a-a407-19fd1e4f117d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for example_img, prompt in example_images_and_prompts:\n",
    "    parameters = {\"prompt\": prompt, \"seed\":1}\n",
    "    guidance_scale_choices = [1,7.5,15,22.5,30]\n",
    "    download_query_parse_response_and_display(example_img, parameters, num_images_per_row=3, \n",
    "                                              override_parameter_choices = {\"guidance_scale\":guidance_scale_choices}, \n",
    "                                             generated_image_display_title = [f\"guidance_scale:{guidance_scale}\" for guidance_scale in guidance_scale_choices]\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b6939-f48f-45fd-9a47-3738a4206922",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.3. Number of steps\n",
    "\n",
    "***\n",
    "Stable diffusion works by iterating the process of reducing the noise with guidance by the prompt, from seemingly random noises. Finally it produce an output image that is human recognizable. As a general rule, a higher number of steps will result in a more detailed image. \n",
    "\n",
    "Take note that higher number of steps will result in longer processing time. Any value above 100 typically will not improve the details of the output further, and the quality may even start to degrade. The recommended setting for Steps would be any value between 10 to 100. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b08ef0-c670-4b4d-97bc-103ca54ba21e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for example_img, prompt in example_images_and_prompts:\n",
    "    parameters = {\"prompt\": prompt, \"seed\":1}\n",
    "    num_inference_steps_choices = [10,30,50,75,100]\n",
    "    download_query_parse_response_and_display(example_img, parameters, num_images_per_row=3, \n",
    "                                              override_parameter_choices = {\"num_inference_steps\":num_inference_steps_choices}, \n",
    "                                             generated_image_display_title = [f\"num_inference_steps:{num_inference_steps}\" for num_inference_steps in num_inference_steps_choices]\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e196e9-cbbf-4ef0-9826-9f6f2724690f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 5.4. Seed\n",
    "***\n",
    "Stable Diffusion generates an output image from noises, and the noises are generated from a seed value. Seed can be useful in many ways. By defining the seed value, you can ensure that your generated images can be replicated with the same prompt and parameters. With the help of a seed, you can also regenerate the image with slightly different prompt or parameters, while keeping the overall composition similar to the initial generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623db926-10ed-4930-95b6-f67e33802cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_images_and_prompts = [\n",
    "    [\"images/input/room.jpg\", \"European style, marble floor, minimalist lifestyle, nature and wood, magical house\"],\n",
    "    [\"images/input/room.jpg\", \"European style, wooden floor, minimalist lifestyle, nature and wood, magical house\"],\n",
    "    [\"images/input/room.jpg\", \"European style, vinyl floor, minimalist lifestyle, nature and wood, magical house\"]\n",
    "]\n",
    "\n",
    "for example_img, prompt in example_images_and_prompts:\n",
    "    parameters = {\"prompt\": prompt, \"strength\": 1}\n",
    "    seed_choices = [100, 101, 102]\n",
    "    download_query_parse_response_and_display(example_img, parameters, num_images_per_row=3, \n",
    "                                              override_parameter_choices = {\"seed\":seed_choices}, \n",
    "                                             generated_image_display_title = [f\"seed:{seed}\" for seed in seed_choices]\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be0e4a6-0252-4e21-99e6-86ecb47b0646",
   "metadata": {},
   "source": [
    "### 6. Clean up the endpoint\n",
    "\n",
    "***\n",
    "After you’re done running the notebook, make sure to delete all resources created in the process to ensure that the billing is stopped.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f075f73-29dc-416c-891e-cb5aedb40d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "model_predictor.delete_model()\n",
    "model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8f4f4-371c-4264-895b-df0e0bf815d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d53be2-2428-46e9-83f9-cca0395edc8a",
   "metadata": {},
   "source": [
    "In the notebook, we have learnt how to deploy Stable Diffusion depth-to-image. We have also explored possible use cases and applications of the model's various parameters. Think out of the box and bring your ideas to life!"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
